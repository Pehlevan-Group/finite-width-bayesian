{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "linear_fcn_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNC41_BSAWqU",
        "outputId": "0c9a90d6-2a5a-4f1e-c40f-c4bc134140b1"
      },
      "source": [
        "!pip install --upgrade --no-deps --force-reinstall -q git+https://github.com/Pehlevan-Group/finite-width-bayesian\n",
        "!pip install neural_tangents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for finite-width-bayesian (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting neural_tangents\n",
            "  Downloading neural_tangents-0.3.8-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.2.18 in /usr/local/lib/python3.7/dist-packages (from neural_tangents) (0.2.21)\n",
            "Collecting frozendict>=1.2\n",
            "  Downloading frozendict-2.0.7-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.18->neural_tangents) (1.15.0)\n",
            "Installing collected packages: frozendict, neural-tangents\n",
            "Successfully installed frozendict-2.0.7 neural-tangents-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uin_nziH_E_w"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "\n",
        "from langevin import model\n",
        "from langevin.utils import convert_nt, curr_time\n",
        "import langevin.theory as theory\n",
        "import langevin.optimizer as opt\n",
        "import langevin.dataset as ds\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit, grad, vmap\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "key = random.PRNGKey(1)\n",
        "\n",
        "from functools import partial\n",
        "from skimage.transform import resize\n",
        "\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "def time_diff(t_start):\n",
        "    t_end = datetime.now(pytz.timezone('US/Eastern'))\n",
        "    t_diff = relativedelta(t_end, t_start)  # later/end time comes first!\n",
        "    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGdDUwij_EJK",
        "outputId": "15c05320-6147-4dda-b821-db701b2f8816"
      },
      "source": [
        "model_type = 'fnn'\n",
        "opt_mode = 'sgld'\n",
        "nonlin = 'linear'\n",
        "dataset_name = 'mnist'\n",
        "resized = 10 ## Resize the images to 10 x 10 pixels\n",
        "\n",
        "N_tr = 1000\n",
        "x_train, y_train = ds.dataset(N_tr, dataset_name, model_type, resized)\n",
        "print(x_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(1000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NoExWM3F_EJQ"
      },
      "source": [
        "## For bottleneck experiments \n",
        "\n",
        "hidden_widths = [[100,100,100],[200,200,200],[300,300,300],[400,400,400],[500,500,500],[600,600,600]]\n",
        "\n",
        "beta = 1\n",
        "batch_size = N_tr\n",
        "step_size = min(1/N_tr, 1e-4)\n",
        "batch_factor = N_tr//batch_size\n",
        "\n",
        "nT = 5000000\n",
        "burn_in = nT//3\n",
        "\n",
        "K_avgs = []\n",
        "K_nngps = []\n",
        "Kernel_Fns = []\n",
        "\n",
        "## Compute the theory\n",
        "K_theories = []\n",
        "for hidden_width in hidden_widths:\n",
        "    print(model_type, ' | ', hidden_width)\n",
        "\n",
        "    ## Create the model layers\n",
        "    layers, layers_ker = model.model(hidden_width, nonlin=nonlin, model_type=model_type)\n",
        "\n",
        "    ## Create the model functions for each layer\n",
        "    layer_fns = []\n",
        "    kernel_fns = []\n",
        "    emp_kernel_fns = []\n",
        "    for i, layer in enumerate(layers):\n",
        "        init_fn, apply_fn, kernel_fn = stax.serial(*(layers[:i+1]))\n",
        "        layer_fns += [jit(apply_fn)]\n",
        "        kernel_fns += [jit(kernel_fn)]\n",
        "        emp_kernel_fns += [jit(partial(nt.empirical_nngp_fn(layer_fns[i]), x_train, None))]\n",
        "    init_fn, apply_fn, kernel_fn = stax.serial(*layers)\n",
        "    apply_fn = jit(apply_fn)\n",
        "    kernel_fn = jit(kernel_fn)\n",
        "    \n",
        "    ## Initialize the model\n",
        "    _, params = init_fn(key, input_shape=x_train.shape)\n",
        "\n",
        "    ## Set Optimizer\n",
        "    opt_init, opt_update, get_params = opt.sgld(step_size, beta, batch_factor)\n",
        "    opt_state = opt_init(params)\n",
        "    \n",
        "    ## Set Loss Function and its grad\n",
        "    loss_fn = jit(lambda params: jnp.sum((apply_fn(params,x_train)-y_train)**2)/2)\n",
        "    g_loss = jit(grad(loss_fn))\n",
        "\n",
        "    avg_count = 0\n",
        "    K_avg = []\n",
        "    t_start = datetime.now(pytz.timezone('US/Eastern'))\n",
        "    for j in range(nT):\n",
        "        _,key = random.split(key)\n",
        "        opt_params = get_params(opt_state)\n",
        "        opt_state = opt_update(i, g_loss(opt_params), opt_state)\n",
        "\n",
        "        if j > burn_in:\n",
        "            avg_count += 1\n",
        "            for i, lay_idx in enumerate(layers_ker):\n",
        "                params = opt_params[:lay_idx+1]\n",
        "                if j == burn_in + 1:\n",
        "                    #K_avg += [nt.empirical_nngp_fn(layer_fns[i])(x_train,None,params)]\n",
        "                    K_avg += [emp_kernel_fns[lay_idx](params)]\n",
        "                else:\n",
        "                    #K_avg[i] += nt.empirical_nngp_fn(layer_fns[i])(x_train,None,params)\n",
        "                    K_avg[i] += emp_kernel_fns[lay_idx](params)\n",
        "\n",
        "        if j % 1000 == 0:\n",
        "            print('%d | loss: %f | avg_count: %d | time: %s'%(j, loss_fn(opt_params), avg_count, time_diff(t_start)), flush=True)\n",
        "            \n",
        "    kernel_fns_relu = []        \n",
        "    K_nngp =  []\n",
        "    for lay_idx in layers_ker:\n",
        "        kernel_fns_relu += [kernel_fns[lay_idx]]\n",
        "        K_nngp += [kernel_fns[lay_idx](x_train,).nngp]\n",
        "    \n",
        "    K_avgs += [K_avg]\n",
        "    K_nngps += [K_nngp]\n",
        "    \n",
        "    ## Compute the theory predictions\n",
        "    _, K_theory, Gxx, Gyy = theory.theory_linear(x_train, y_train, beta, kernel_fns, hidden_width)\n",
        "    K_theories += [K_theory]\n",
        "        \n",
        "    with open('data_%s_%d_%s_%s_%s_nT_%d.pkl'%(str(hidden_width), N_tr, model_type, opt_mode, nonlin, nT), 'wb') as outfile:\n",
        "        pickle.dump({'K_avg': K_avg, 'K_nngp': K_nngp, 'K_theory': K_theory, 'burn_in': burn_in, \n",
        "                 'model_type': model_type, 'hidden_widths': hidden_widths, 'N_tr': N_tr, \n",
        "                 'nT': nT, 'beta': beta, 'batch_size': batch_size, 'step_size': step_size,\n",
        "                 'avg_count': avg_count, 'opt_mode': opt_mode}, outfile, pickle.HIGHEST_PROTOCOL)\n",
        "                 \n",
        "\n",
        "    plt.scatter((K_avg[0]/avg_count-Gxx).reshape(-1)[:], (K_theory[0]-Gxx).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n",
        "    plt.savefig('k-nngp_%s_fnn_%s.jpg'%(str(hidden_width), opt_mode))\n",
        "    plt.show()\n",
        "    \n",
        "    plt.scatter((K_avg[0]/avg_count).reshape(-1)[:], (K_theory[0]).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n",
        "    plt.savefig('k_vs_nngp_%s_fnn_%s.jpg'%(str(hidden_width), opt_mode))\n",
        "    plt.show()\n",
        "\n",
        "        \n",
        "with open('data_%d_%s_%s.pkl'%(N_tr, model_type, opt_mode), 'wb') as outfile:\n",
        "    pickle.dump({'K_avgs': K_avgs, 'K_nngps': K_nngps, 'K_theories': K_theories, 'nonlin': nonlin,\n",
        "                 'model_type': model_type, 'hidden_widths': hidden_widths, 'N_tr': N_tr, \n",
        "                 'nT': nT, 'beta': beta, 'batch_size': batch_size, 'step_size': step_size,\n",
        "                 'avg_count': avg_count, 'opt_mode': opt_mode}, outfile, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls-EMLE0_EJT"
      },
      "source": [
        "depths = np.arange(len(K_avgs[0]))\n",
        "deviation = []\n",
        "deviation_th = []\n",
        "\n",
        "for i, hidden_width in enumerate(hidden_widths):\n",
        "    width = hidden_width[0]\n",
        "    K_exp = K_avgs[i]\n",
        "    K_nngp = K_nngps[i]\n",
        "    deviation_width = []\n",
        "    for j, K in enumerate(K_exp):\n",
        "        K_th = K_nngp[j]\n",
        "        deviation_width += [np.linalg.norm((K/avg_count - K_th))**2]\n",
        "    deviation += [deviation_width]\n",
        "    \n",
        "    K_theory = K_theories[i]\n",
        "    deviation_th += [[np.linalg.norm(K - K_t)**2 for K, K_t in zip(K_theory, K_nngp)]]\n",
        "\n",
        "deviation = np.array(deviation)\n",
        "print(deviation.shape)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "for l in range(2):\n",
        "    plt.loglog([width[0] for width in hidden_widths], deviation[:,l], 'o', label='layer %d'%(l+1))\n",
        "\n",
        "if model_type == 'fnn':\n",
        "    deviation_th = np.array(deviation_th)\n",
        "    for l in range(2):\n",
        "        if l == 0:\n",
        "            plt.loglog([width[0] for width in hidden_widths], deviation_th[:,l],'k--', label='theory')\n",
        "        else:\n",
        "            plt.loglog([width[0] for width in hidden_widths], deviation_th[:,l],'k--')\n",
        "\n",
        "\n",
        "plt.gca().tick_params(axis='both', which = 'major', labelsize=14)\n",
        "plt.gca().tick_params(axis='both', which = 'minor', labelsize=12)\n",
        "plt.legend(fontsize=16)    \n",
        "plt.xlabel(r'Width', fontsize=20)\n",
        "plt.ylabel(r'$||K_{exp} - K_{GP}||_F$', fontsize=20)\n",
        "plt.tight_layout()\n",
        "# plt.gca().set_aspect(0.28)\n",
        "plt.savefig('one_over_width_%s_sgld.png'%model_type, dpi=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc6rt5oH_EJT"
      },
      "source": [
        "data_limit = 2000\n",
        "\n",
        "for j in range(len(hidden_widths[0])):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for i, hidden_width in enumerate(hidden_widths):\n",
        "        k_exp = (K_avgs[i][j]/avg_count-Gxx).reshape(-1)[:data_limit]\n",
        "        k_th  = (K_theories[i][j]-Gxx).reshape(-1)[:data_limit]\n",
        "        \n",
        "        lin = np.linspace(min(k_th),max(k_th),1000)\n",
        "        print(k_th.shape)\n",
        "        plt.scatter(k_exp, k_th, label='Width: %d'%hidden_width[j])\n",
        "        if i == 0:\n",
        "            plt.plot(lin,lin,'k--')\n",
        "            plt.xlim([min(lin)*1.2,max(lin)*1.05])\n",
        "            plt.ylim([min(lin)*1.2,max(lin)*1.05])\n",
        "        plt.legend(fontsize=16)   \n",
        "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
        "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xlabel('$K_{exp}^{(%d)} - K_{GP}^{(%d)}$'%(j + 1,j + 1), fontsize=20)\n",
        "    plt.ylabel('$K_{th}^{(%d)} - K_{GP}^{(%d)}$'%(j + 1,j + 1), fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('k-nngp_fnn_sgld_layer_%d.png'%(j+1), dpi=600)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}