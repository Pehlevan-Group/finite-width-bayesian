{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "conv1D_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGyC5GpDPAoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d0a1d3-2317-461e-945e-e0f04adfc9cc"
      },
      "source": [
        "!pip install --upgrade --no-deps --force-reinstall -q git+https://github.com/Pehlevan-Group/finite-width-bayesian\n",
        "!pip install neural_tangents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for finite-width-bayesian (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting neural_tangents\n",
            "  Downloading neural_tangents-0.3.8-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting frozendict>=1.2\n",
            "  Downloading frozendict-2.0.7-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: jax>=0.2.18 in /usr/local/lib/python3.7/dist-packages (from neural_tangents) (0.2.21)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (3.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.18->neural_tangents) (1.15.0)\n",
            "Installing collected packages: frozendict, neural-tangents\n",
            "Successfully installed frozendict-2.0.7 neural-tangents-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5gv6vy0O1ix"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "\n",
        "from langevin import model\n",
        "from langevin.utils import convert_nt, curr_time\n",
        "import langevin.theory as theory\n",
        "import langevin.optimizer as opt\n",
        "import langevin.dataset as ds\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jit, grad, vmap\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "key = random.PRNGKey(1)\n",
        "\n",
        "from functools import partial\n",
        "from skimage.transform import resize\n",
        "\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "def time_diff(t_start):\n",
        "    t_end = datetime.now(pytz.timezone('US/Eastern'))\n",
        "    t_diff = relativedelta(t_end, t_start)  # later/end time comes first!\n",
        "    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAmYfZtnO1i1",
        "outputId": "bc193fae-b274-4ec8-b8ab-92c5f18f684f"
      },
      "source": [
        "dataset_name = 'mnist'\n",
        "model_type = 'cnn1d'\n",
        "opt_mode = 'sgld'\n",
        "nonlin = 'linear'\n",
        "\n",
        "N_tr = 50\n",
        "resized = 5\n",
        "x_train, y_train = ds.dataset(N_tr, dataset_name, model_type, resized);\n",
        "print(x_train.shape)\n",
        "\n",
        "hidden_widths = [[250,250], [400,400], [500,500], [600,600], [700,700], [750,750]]\n",
        "beta = 1\n",
        "batch_size = N_tr\n",
        "step_size = 1/2000\n",
        "batch_factor = N_tr//batch_size\n",
        "\n",
        "## Set this to 2000000 to obtain accurate posterior mean\n",
        "nT = 2000\n",
        "burn_in = nT//4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(50, 25, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fGoOJe72O1i3"
      },
      "source": [
        "K_avgs = []\n",
        "K_nngps = []\n",
        "\n",
        "## Compute the theory\n",
        "K_theories = []\n",
        "for hidden_width in hidden_widths:\n",
        "    print(model_type, ' | ', hidden_width)\n",
        "\n",
        "    ## Create the model layers\n",
        "    layers, layers_ker = model.model(hidden_width, nonlin = nonlin, model_type = model_type)\n",
        "    ## Create the model functions for each layer\n",
        "    init_fn, apply_fn, kernel_fn, layer_fns, kernel_fns, emp_kernel_fns = model.network_fns(layers, x_train)\n",
        "    ## Initialize the model\n",
        "    _, params = init_fn(key, input_shape=x_train.shape)\n",
        "\n",
        "    ## Set Optimizer\n",
        "    opt_init, opt_update, get_params = opt.sgld(step_size, beta, batch_factor)\n",
        "    opt_state = opt_init(params)\n",
        "    \n",
        "    ## Set Loss Function and its grad\n",
        "    loss_fn = jit(lambda params: jnp.sum((apply_fn(params,x_train)-y_train)**2)/2)\n",
        "    g_loss = jit(grad(loss_fn))\n",
        "\n",
        "    avg_count = 0\n",
        "    K_avg = []\n",
        "    t_start = curr_time()\n",
        "    for j in range(nT):\n",
        "        opt_params = get_params(opt_state)\n",
        "        opt_state = opt_update(j, g_loss(opt_params), opt_state)\n",
        "\n",
        "        if j > burn_in:\n",
        "            avg_count += 1\n",
        "            for i, idx in enumerate(layers_ker):\n",
        "                if j == burn_in + 1:\n",
        "                    K_avg += [emp_kernel_fns[idx](opt_params[:idx+1])]\n",
        "                else: \n",
        "                    K_avg[i] += emp_kernel_fns[idx](opt_params[:idx+1])\n",
        "\n",
        "        if j % 1000 == 0:\n",
        "            print('%d | loss: %f | avg_count: %d | time: %s'%(j, loss_fn(opt_params), avg_count, time_diff(t_start)), flush=True)\n",
        "    \n",
        "    K_nngp, K_theory, Gxx, Gyy = theory.theory_linear(x_train, y_train, beta, kernel_fns, hidden_width)\n",
        "    K_nngps += [K_nngp]\n",
        "    K_theories += [K_theory]\n",
        "\n",
        "        \n",
        "    with open('data_%s_%d_%s_%s_%s.pkl'%(str(hidden_width), N_tr, model_type, opt_mode, nonlin), 'wb') as outfile:\n",
        "        pickle.dump({'K_avg': K_avg, 'K_nngp': K_nngp, 'K_theory': K_theory, \n",
        "                 'model_type': model_type, 'hidden_widths': hidden_widths, 'N_tr': N_tr, \n",
        "                 'nT': nT, 'beta': beta, 'batch_size': batch_size, 'step_size': step_size,\n",
        "                 'avg_count': avg_count, 'opt_mode': opt_mode}, outfile, pickle.HIGHEST_PROTOCOL)\n",
        "                 \n",
        "\n",
        "    if model_type == 'fnn':\n",
        "        plt.scatter((K_avg[0]/avg_count-Gxx).reshape(-1)[:], (K_theory[0]-Gxx).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n",
        "        plt.savefig('k-nngp_%s_fnn_%s_%s.jpg'%(str(hidden_width), opt_mode, nonlin))\n",
        "        plt.close()\n",
        "    \n",
        "        plt.scatter((K_avg[0]/avg_count).reshape(-1)[:], (K_theory[0]).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n",
        "        plt.savefig('k_vs_nngp_%s_fnn_%s_%s.jpg'%(str(hidden_width), opt_mode, nonlin))\n",
        "        plt.close()\n",
        "\n",
        "        \n",
        "with open('data_%d_%s_%s_%s.pkl'%(N_tr, model_type, opt_mode, nonlin), 'wb') as outfile:\n",
        "    pickle.dump({'K_avgs': K_avgs, 'K_nngps': K_nngps, 'K_theories': K_theories, \n",
        "                 'model_type': model_type, 'hidden_widths': hidden_widths, 'N_tr': N_tr, \n",
        "                 'nT': nT, 'beta': beta, 'batch_size': batch_size, 'step_size': step_size,\n",
        "                 'avg_count': avg_count, 'opt_mode': opt_mode}, outfile, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LqJVK4SZYYu"
      },
      "source": [
        "K_avgs = []\n",
        "K_nngps = []\n",
        "avg_counts = []\n",
        "nTs = []\n",
        "\n",
        "nonlin = 'linear'\n",
        "data_files = ['data_%s_%d_%s_%s_%s.pkl'%(str(s), N_tr, model_type, opt_mode, nonlin) for s in hidden_widths]\n",
        "\n",
        "for data_file in data_files:\n",
        "    with open(data_file, 'rb') as infile:\n",
        "        data = pickle.load(infile)\n",
        "        N_tr = data['N_tr']\n",
        "        model_type= data['model_type']\n",
        "        K_avgs += [data['K_avg']]\n",
        "        K_nngps += [data['K_nngp']]\n",
        "        avg_counts += [data['avg_count']]\n",
        "        nTs += [data['nT']]\n",
        "        beta = data['beta']\n",
        "\n",
        "## Preprocess NT kernels\n",
        "for i, K_width in zip(np.arange(len(K_avgs)), K_avgs):\n",
        "    for j, K in enumerate(K_width):\n",
        "        K_avgs[i][j] =  convert_nt(K)/avg_count"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZu_sHxnO1i5"
      },
      "source": [
        "depths = jnp.arange(len(K_avgs[0]))\n",
        "deviation = []\n",
        "deviation_th = []\n",
        "\n",
        "for K_exp, K_nngp, K_theory, hidden_width in zip(K_avgs, K_nngps, K_theories, hidden_widths):\n",
        "\n",
        "    dev_width = []\n",
        "    dev_width_th = []\n",
        "    for j, K, K_nn, K_th in zip(np.arange(len(K_exp)), K_exp, K_nngp, [*K_theory,0,0]):\n",
        "        \n",
        "        if len(K.shape) == 6:\n",
        "            K = K.reshape(N_tr, N_tr, 100, 100)\n",
        "            K_nn = K_nn.reshape(N_tr, N_tr, 100, 100)\n",
        "            K_th = K_th.reshape(N_tr, N_tr, 100, 100)\n",
        "            \n",
        "        dev_width += [jnp.linalg.norm(K - K_nn)]\n",
        "        dev_width_th += [jnp.linalg.norm((K_th - K_nn))]\n",
        "        \n",
        "        \n",
        "    deviation += [dev_width]\n",
        "    deviation_th += [dev_width_th]\n",
        "\n",
        "deviation = np.array(deviation)\n",
        "deviation_th = np.array(deviation_th)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afApiWdThUbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178e6042-bce7-42c6-bc0d-d6bb89cad0e5"
      },
      "source": [
        "lay_idx = 0\n",
        "xs = []\n",
        "widths = [width[0] for width in hidden_widths[:]]\n",
        "pix_size = 6000\n",
        "\n",
        "plt.figure(figsize=(6,5.5))\n",
        "for width_idx in range(len(hidden_widths)):\n",
        "    K_exp = [K.reshape(N_tr,N_tr,resized**2,resized**2) for K in K_avgs[width_idx][:2]]\n",
        "    K_nngp_th = [K.reshape(N_tr,N_tr,resized**2,resized**2) for K in K_nngp[:2]]\n",
        "    K_th = [K.reshape(N_tr,N_tr,resized**2,resized**2) for K in K_theories[width_idx]]\n",
        "\n",
        "    x = (K_exp[lay_idx]-K_nngp_th[lay_idx]).reshape(-1)[:pix_size]\n",
        "    y = (K_th[lay_idx]-K_nngp_th[lay_idx]).reshape(-1)[:pix_size]\n",
        "    plt.scatter(x, y, label='width = %d'%widths[width_idx])\n",
        "    if width_idx == 0:\n",
        "        plt.plot(np.linspace(min(x), max(x), 1000), np.linspace(min(x), max(x),1000),'k--')\n",
        "    print(np.mean(K_exp[lay_idx]/K_th[lay_idx]), np.mean(x/y))\n",
        "plt.plot(x,x, 'k--')     \n",
        "plt.legend(fontsize=12)\n",
        "plt.gca().tick_params(axis='both', which = 'major', labelsize=14)\n",
        "plt.gca().tick_params(axis='both', which = 'minor', labelsize=14)\n",
        "plt.xlabel('$K_{exp}^{(%d)} - K_{GP}^{(%d)}$'%(lay_idx + 1,lay_idx + 1), fontsize=20)\n",
        "plt.ylabel('$K_{th}^{(%d)} - K_{GP}^{(%d)}$'%(lay_idx + 1,lay_idx + 1), fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('k-nngp_cov_cnn1d_layer_%d.png'%(lay_idx+1), dpi=600)\n",
        "plt.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9710410164304019 2.1609058940264463\n",
            "0.9257817125812909 11.009963712609785\n",
            "1.2664631569170828 3.35836258084833\n",
            "1.07996555025582 5.873307340313838\n",
            "0.9494100220237902 7.272438913285475\n",
            "1.0288474503046459 -0.4219553860220873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb651xQzYvi_"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}