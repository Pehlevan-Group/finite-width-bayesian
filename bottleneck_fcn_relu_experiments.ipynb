{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"bottleneck_fcn_relu_experiments.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNC41_BSAWqU","executionInfo":{"status":"ok","timestamp":1635461797463,"user_tz":240,"elapsed":5738,"user":{"displayName":"Abdulkadir Canatar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJhu2_WKdfF0J-AxPhvEIfXqsD3xVQWWPxxbV2-A=s64","userId":"15364711240135652230"}},"outputId":"f419ac9a-1111-4a73-d41c-4702888217ff"},"source":["!pip install --upgrade --no-deps --force-reinstall -q git+https://github.com/Pehlevan-Group/finite-width-bayesian\n","!pip install neural_tangents"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["  Building wheel for finite-width-bayesian (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: neural_tangents in /usr/local/lib/python3.7/dist-packages (0.3.8)\n","Requirement already satisfied: frozendict>=1.2 in /usr/local/lib/python3.7/dist-packages (from neural_tangents) (2.0.7)\n","Requirement already satisfied: jax>=0.2.18 in /usr/local/lib/python3.7/dist-packages (from neural_tangents) (0.2.21)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (1.19.5)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (0.12.0)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (1.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->neural_tangents) (3.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.18->neural_tangents) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"Uin_nziH_E_w","executionInfo":{"status":"ok","timestamp":1635461800607,"user_tz":240,"elapsed":3148,"user":{"displayName":"Abdulkadir Canatar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJhu2_WKdfF0J-AxPhvEIfXqsD3xVQWWPxxbV2-A=s64","userId":"15364711240135652230"}}},"source":["import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import neural_tangents as nt\n","from neural_tangents import stax\n","\n","from langevin import model\n","from langevin.utils import convert_nt, curr_time\n","import langevin.theory as theory\n","import langevin.optimizer as opt\n","import langevin.dataset as ds\n","\n","import jax\n","import jax.numpy as jnp\n","from jax import random\n","from jax import jit, grad, vmap\n","from jax.config import config\n","config.update(\"jax_enable_x64\", True)\n","key = random.PRNGKey(1)\n","\n","from functools import partial\n","from skimage.transform import resize\n","\n","import pytz\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","\n","def time_diff(t_start):\n","    t_end = datetime.now(pytz.timezone('US/Eastern'))\n","    t_diff = relativedelta(t_end, t_start)  # later/end time comes first!\n","    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGdDUwij_EJK","executionInfo":{"status":"ok","timestamp":1635461801537,"user_tz":240,"elapsed":935,"user":{"displayName":"Abdulkadir Canatar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJhu2_WKdfF0J-AxPhvEIfXqsD3xVQWWPxxbV2-A=s64","userId":"15364711240135652230"}},"outputId":"02701a5b-91c2-4fea-9d76-477a3ef18c7a"},"source":["model_type = 'fnn'\n","opt_mode = 'sgld'\n","nonlin = 'relu'\n","dataset_name = 'mnist'\n","resized = 10 ## Resize the images to 10 x 10 pixels\n","\n","N_tr = 1000\n","x_train, y_train = ds.dataset(N_tr, dataset_name, model_type, resized)\n","print(x_train.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(1000, 100)\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NoExWM3F_EJQ"},"source":["## For bottleneck experiments \n","\n","no_bottleneck_widths = [[100,100,100],[200,200,200],[300,300,300],[400,400,400],[500,500,500],[600,600,600]]\n","bottleneck_widths = [[100,50,100],[200,50,200],[300,50,300],[400,50,400],[500,50,500],[600,50,600]]\n","\n","exp_type = 0 # set it to 1 for bottleneck experiments\n","\n","if exp_type == 0:\n","  hidden_widths = no_bottleneck_widths\n","else:\n","  hidden_widths = bottleneck_widths\n","\n","\n","beta = 1\n","batch_size = N_tr\n","step_size = min(1/N_tr, 1e-4)\n","batch_factor = N_tr//batch_size\n","\n","nT = 5000000\n","burn_in = nT//3\n","\n","K_avgs = []\n","K_nngps = []\n","Kernel_Fns = []\n","\n","## Compute the theory\n","K_theories = []\n","for hidden_width in hidden_widths:\n","    print(model_type, ' | ', hidden_width)\n","\n","    ## Create the model layers\n","    layers, layers_ker = model.model(hidden_width, nonlin=nonlin, model_type=model_type)\n","\n","    ## Create the model functions for each layer\n","    layer_fns = []\n","    kernel_fns = []\n","    emp_kernel_fns = []\n","    for i, layer in enumerate(layers):\n","        init_fn, apply_fn, kernel_fn = stax.serial(*(layers[:i+1]))\n","        layer_fns += [jit(apply_fn)]\n","        kernel_fns += [jit(kernel_fn)]\n","        emp_kernel_fns += [jit(partial(nt.empirical_nngp_fn(layer_fns[i]), x_train, None))]\n","    init_fn, apply_fn, kernel_fn = stax.serial(*layers)\n","    apply_fn = jit(apply_fn)\n","    kernel_fn = jit(kernel_fn)\n","    \n","    ## Initialize the model\n","    _, params = init_fn(key, input_shape=x_train.shape)\n","\n","    ## Set Optimizer\n","    opt_init, opt_update, get_params = opt.sgld(step_size, beta, batch_factor)\n","    opt_state = opt_init(params)\n","    \n","    ## Set Loss Function and its grad\n","    loss_fn = jit(lambda params: jnp.sum((apply_fn(params,x_train)-y_train)**2)/2)\n","    g_loss = jit(grad(loss_fn))\n","\n","    avg_count = 0\n","    K_avg = []\n","    t_start = datetime.now(pytz.timezone('US/Eastern'))\n","    for j in range(nT):\n","        _,key = random.split(key)\n","        opt_params = get_params(opt_state)\n","        opt_state = opt_update(i, g_loss(opt_params), opt_state)\n","\n","        if j > burn_in:\n","            avg_count += 1\n","            for i, lay_idx in enumerate(layers_ker):\n","                params = opt_params[:lay_idx+1]\n","                if j == burn_in + 1:\n","                    #K_avg += [nt.empirical_nngp_fn(layer_fns[i])(x_train,None,params)]\n","                    K_avg += [emp_kernel_fns[lay_idx](params)]\n","                else:\n","                    #K_avg[i] += nt.empirical_nngp_fn(layer_fns[i])(x_train,None,params)\n","                    K_avg[i] += emp_kernel_fns[lay_idx](params)\n","\n","        if j % 1000 == 0:\n","            print('%d | loss: %f | avg_count: %d | time: %s'%(j, loss_fn(opt_params), avg_count, time_diff(t_start)), flush=True)\n","            \n","    kernel_fns_relu = []        \n","    K_nngp =  []\n","    for lay_idx in layers_ker:\n","        kernel_fns_relu += [kernel_fns[lay_idx]]\n","        K_nngp += [kernel_fns[lay_idx](x_train,).nngp]\n","    \n","    K_avgs += [K_avg]\n","    K_nngps += [K_nngp]\n","    \n","    ## Compute the theory predictions\n","    if model_type == 'fnn':\n","        _, K_theory, Gxx, Gyy = theory.theory_linear(x_train, y_train, beta, kernel_fns, hidden_width)\n","        K_theories += [K_theory]\n","        \n","    with open('data_%s_%d_%s_%s_%s_nT_%d.pkl'%(str(hidden_width), N_tr, model_type, opt_mode, nonlin, nT), 'wb') as outfile:\n","        pickle.dump({'K_avg': K_avg, 'K_nngp': K_nngp, 'K_theory': K_theory, 'burn_in': burn_in, \n","                 'model_type': model_type, 'hidden_widths': hidden_widths, 'N_tr': N_tr, \n","                 'nT': nT, 'beta': beta, 'batch_size': batch_size, 'step_size': step_size,\n","                 'avg_count': avg_count, 'opt_mode': opt_mode}, outfile, pickle.HIGHEST_PROTOCOL)\n","                 \n","\n","    plt.scatter((K_avg[0]/avg_count-Gxx).reshape(-1)[:], (K_theory[0]-Gxx).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n","    plt.savefig('k-nngp_%s_fnn_%s.jpg'%(str(hidden_width), opt_mode))\n","    plt.show()\n","    \n","    plt.scatter((K_avg[0]/avg_count).reshape(-1)[:], (K_theory[0]).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n","    plt.savefig('k_vs_nngp_%s_fnn_%s.jpg'%(str(hidden_width), opt_mode))\n","    plt.show()\n","\n","        \n","with open('data_%d_%s_%s.pkl'%(N_tr, model_type, opt_mode), 'wb') as outfile:\n","    pickle.dump({'K_avgs': K_avgs, 'K_nngps': K_nngps, 'K_theories': K_theories, 'nonlin': nonlin,\n","                 'model_type': model_type, 'hidden_widths': hidden_widths, 'N_tr': N_tr, \n","                 'nT': nT, 'beta': beta, 'batch_size': batch_size, 'step_size': step_size,\n","                 'avg_count': avg_count, 'opt_mode': opt_mode}, outfile, pickle.HIGHEST_PROTOCOL)\n","\n","depths = jnp.arange(len(K_avgs[0]))\n","deviation = []\n","deviation_th = []\n","\n","for i, hidden_width in enumerate(hidden_widths):\n","    width = hidden_width[0]\n","    K_exp = K_avgs[i]\n","    K_nngp = K_nngps[i]\n","    deviation += [[jnp.linalg.norm(K/avg_count - K_t)**2 for K, K_t in zip(K_exp, K_nngp)]]\n","\n","    if model_type == 'fnn':\n","        K_theory = K_theories[i]\n","        deviation_th += [[jnp.linalg.norm(K - K_t)**2 for K, K_t in zip(K_theory, K_nngp)]]\n","\n","deviation = np.array(deviation)\n","print(deviation.shape)\n","plt.loglog([width[0] for width in hidden_widths], deviation[:,:-1], 'o')\n","\n","if model_type == 'fnn':\n","    deviation_th = np.array(deviation_th)\n","    plt.loglog([width[0] for width in hidden_widths], deviation_th,'k--')\n","    \n","plt.savefig('one_over_width_%s_%s.png'%(model_type, opt_mode))\n","plt.close()\n","\n","for i, hidden_width in enumerate(hidden_widths):\n","    plt.scatter((K_avgs[i][0]/avg_count-Gxx).reshape(-1)[:], (K_theories[i][0]-Gxx).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n","    #plt.legend()\n","    plt.savefig('k-nngp_fnn_%s.jpg'%opt_mode)\n","plt.close()\n","\n","for i, hidden_width in enumerate(hidden_widths):\n","    plt.scatter((K_avgs[i][0]/avg_count).reshape(-1)[:], (K_theories[i][0]).reshape(-1)[:], label='Width: %d'%hidden_width[0])\n","    plt.savefig('k_vs_nngp_fnn_%s.jpg'%opt_mode)\n","plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls-EMLE0_EJT","executionInfo":{"status":"ok","timestamp":1635463623681,"user_tz":240,"elapsed":142,"user":{"displayName":"Abdulkadir Canatar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJhu2_WKdfF0J-AxPhvEIfXqsD3xVQWWPxxbV2-A=s64","userId":"15364711240135652230"}}},"source":[""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tc6rt5oH_EJT","executionInfo":{"status":"ok","timestamp":1635463624201,"user_tz":240,"elapsed":2,"user":{"displayName":"Abdulkadir Canatar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhJhu2_WKdfF0J-AxPhvEIfXqsD3xVQWWPxxbV2-A=s64","userId":"15364711240135652230"}}},"source":[""],"execution_count":5,"outputs":[]}]}